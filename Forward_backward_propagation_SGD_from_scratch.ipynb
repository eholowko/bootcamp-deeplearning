{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Propagacja wsteczna "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Przy większej ilości wag rozpisywanie każdej pochodnej z osobna byłoby uciążliwe. Dlatego wygodniej jest stosować notację macierzową.  \n",
    "\n",
    "Niech $X$ będzie macierzą danych o wymiarach $n \\times m$, $W$ będzie macierzą wag $m \\times h$ a $b$ będzie macierzą wyrazów wolnych $n \\times h$. Wtedy nasz model jednowarstwowy można zapisać jako \n",
    "\n",
    "$$ Y = XW + b $$\n",
    "\n",
    "$Y$ jest teraz macierzą zawierającą aktywacje $h$ neuronów dla każdej z $n$ obserwacji w zbiorze danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tak zdefiniowaną warstwę nazywamy warstwą *** fully connected *** (analogicznie *** dense *** )."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jeśli nasza sieć ma tylko tą jedną warstwę, to $Y$ jest wynikiem naszej sieci, i policzymy na nim naszą funkcję straty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zadajmy sobie pytanie, które było głównym motorem napędowym perceptronu: **jak nasze wagi $W$ wpływają na funkcję straty $L$**?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczywiście możemy to zrobić tak jak wcześniej - policzyć pochodną $L$ po wszystkich elementach macierzy $W$. Jednak byłoby to dosć uciążliwe - zarówno obliczeniowo, jak i pod kątem zapisu.  \n",
    "\n",
    "Możemy zrobić to analogicznie do tego, co widzieliśmy w perceptronie - najpierw policzyć pochodną funkcji straty $L$ po $Y$, a następnie pochodną $Y$ po $W$. Wtedy, z reguły łancuchowej dostajemy:\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial W} =  \\frac{\\partial L}{\\partial Y}  \\frac{\\partial Y}{\\partial W} $$\n",
    "<br>\n",
    "Pochodną funkcji straty $L$ po $Y$ powinniśmy mieć \"za darmo\" - sami definiujemy funkcję straty, więc powinniśmy wiedzieć ile wynosi jej pochodna."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytanie tylko, jak obliczyć pochodną $Y$ po $W$.\n",
    "\n",
    "Okazuje się, że różniczkowanie na takich macierzach wygląda bardzo podobnie do różniczkowania zwykłych skalarów.  \n",
    "Gdyby $y$, $x$, $w$, oraz $b$ były liczbami, to moglibyśmy zapisać\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial w} = \\frac{\\partial (xw + b)}{\\partial w} = x $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Użyjmy więc tej samej notacji dla macierzy:\n",
    "\n",
    "$$ \\frac{\\partial Y}{\\partial W} = \\frac{\\partial (XW + b)}{\\partial W} = X $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analogicznie,\n",
    "\n",
    "$$ \\frac{\\partial Y}{\\partial X} = \\frac{\\partial (XW + b)}{\\partial X} = W $$\n",
    "\n",
    "oraz\n",
    "\n",
    "$$ \\frac{\\partial Y}{\\partial b} = \\frac{\\partial (XW + b)}{\\partial b} = I $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oczywiście, jest to dość mocne uproszczenie, ale dobrze przekłada się na intuicję wyciągniętą z analizy jednowymiarowej. Dokładniejsze uzasadnienie znajdziecie na http://cs231n.stanford.edu/vecDerivs.pdf .\n",
    "\n",
    "W praktyce można opierać się o te wzory, dokładając ewentualnie transpozycję tak, aby zgadzały się wymiary macierzy podczas mnożenia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takie działanie nazywa się **metodą propagacji wstecznej** (***backpropagation***)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy więc przepis, jak zbudować sieć opartą o operacje na macierzach. Spróbujmy napisać teraz kilka funkcji które to zrealizują."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementacja sieci bez warstw ukrytych za pomocą notacji macierzowej"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** - zaimplementuj funkcję `fc_forward`. Wskazówka - przy użyciu `np.dot` są to dwie linijki (a jakby się uprzeć, nawet jedna). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_forward(x, w, b):\n",
    "    \"\"\"\n",
    "    Funkcja oblicza 'forward pass' w warstwie typu 'fully connected'. \n",
    "    \n",
    "    Argumenty:\n",
    "    - x: Dane wejściowe - np.array o dwóch wymiarach (N,M)\n",
    "    - w: macierz wag w warstwie - np.array o dwóch wymiarach (M, M_out)\n",
    "    - b: wektor 'bias' - np.array o wymiarze (M_out,)\n",
    "    \n",
    "    Wartość zwracana:\n",
    "    - out: wyjście z warstwy\n",
    "    - cache: krotka zawierająca argumenty wejściowe\n",
    "    \"\"\"\n",
    "    y = np.dot(x,w) + b\n",
    "    \n",
    "    return y,(x,w,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fc_backward(dy, cache):\n",
    "    \"\"\"\n",
    "    Funkcja oblicza 'backward pass' (propagację gradientu) w warstwie typu 'fully connected'. \n",
    "    Na wyjściu dostajemy pochodną funkcji straty po danych wejściowych (x),\n",
    "    wagach (w) oraz wartościach stałych (b).\n",
    "    \n",
    "    Argumenty:\n",
    "    - dy: pochodna wejściowa (czyli pochodna funkcji straty po warstwie leżącej po aktualnej) \n",
    "        - np.array o wymiarze (N, M_out)\n",
    "    - cache: krotka zwrócona przez fc_forward\n",
    "    \n",
    "    Wartość zwracana:\n",
    "    - dx: pochodna po elementach x - np.array o wymiarze (N, M)\n",
    "    - dw: pochodna po elementach w - np.array o wymiarze (M, M_out)\n",
    "    - db: pochodna po elementach b - np.array o wymiarze (M_out,)\n",
    "    \"\"\"\n",
    "    \n",
    "    x,w,b = cache\n",
    "    \n",
    "    dx = np.dot(dy,w.T)\n",
    "    dw = np.dot(x.T,dy)\n",
    "    db = np.sum(dy,axis=0)\n",
    "    \n",
    "    return dx, dw, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie** - zaimplelentuj funkcję, która obliczy MSE dla danych wektorów `y_pred`, `y_true`, oraz zwróci pochodną MSE po predykowanych wartościach.  \n",
    "\n",
    "Funkcja ma działać wektorowo - tzn ma przyjąć na wejściu dwa wektory (z czego każdy reprezentuje predykcję/prawdziwą wartość dla każdej obserwacji w zbiorze), i zwrócić skalar z wartością funkcji straty oraz wektor z pochodnymi funkcji straty dla każdej obserwacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mse_loss_with_gradient(y_pred, y_true):\n",
    "    n = y_true.shape[0]\n",
    "    \n",
    "    delta_y = y_pred - y_true\n",
    "    \n",
    "    return (np.mean(delta_y * delta_y),2 * (delta_y) / n)# dla Python2 dzielenie całkowite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate data\n",
    "X = np.random.normal(size=(1000, 3))\n",
    "y = np.sum(X * np.array([0.4, 2, -3]), axis=1, keepdims=True) -3 + np.random.normal(scale=0.5, size=(1000, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w = np.zeros(shape=(3, 1))\n",
    "b = np.zeros(shape=(1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# napiszmy własną sieć neuronową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 22.27519620376568\n",
      "Total loss 14.414899078609785\n",
      "Total loss 9.36399526903167\n",
      "Total loss 6.115996353214289\n",
      "Total loss 4.025830933057914\n",
      "Total loss 2.679771095797807\n",
      "Total loss 1.812275115815748\n",
      "Total loss 1.2527881248913908\n",
      "Total loss 0.8916847890225518\n",
      "Total loss 0.658451191098835\n",
      "Total loss 0.5076976578701686\n",
      "Total loss 0.4101854708109279\n",
      "Total loss 0.34706603487859256\n",
      "Total loss 0.3061797620325975\n",
      "Total loss 0.27967651279512357\n",
      "Total loss 0.2624845881102806\n",
      "Total loss 0.2513249557730853\n",
      "Total loss 0.24407607164750655\n",
      "Total loss 0.23936430406205544\n",
      "Total loss 0.23629963746294155\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.1\n",
    "w = np.zeros(shape=(3, 1))\n",
    "b = np.zeros(shape=(1,))\n",
    "\n",
    "for i in range(20):\n",
    "    #forward pass\n",
    "    out,cache = fc_forward(X, w, b)\n",
    "    \n",
    "    loss, dloss = mse_loss_with_gradient(out,y)\n",
    "    \n",
    "    #backward pass\n",
    "    dx, dw, db = fc_backward(dloss,cache)\n",
    "    \n",
    "    w -= dw * learning_rate\n",
    "    b -= db * learning_rate\n",
    "    \n",
    "    print('Total loss %s' %loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.40857708],\n",
       "        [ 2.00099756],\n",
       "        [-2.96581573]]), array([-2.96815608]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w,b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bardzo często spotkamy się z sytuacjami, w których nie jesteśmy w stanie wykonać operacji mnożenia macierzowego na całej macierzy wejściowej. Wtedy rozwiązaniem jest **Stochastic Gradient Descent - SGD**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Cost(\\theta) = \\frac{1}{n} \\sum\\limits_{i=1}^n f(x_i,y_i,\\theta)$$\n",
    "\n",
    "$$ \\tilde{Cost}(\\theta) = \\frac{1}{r} \\sum\\limits_{i \\in \\{ i_1, ..., i_r \\} } f(x_i,y_i,\\theta),$$ $$ \\ \\ \\text{gdzie} \\ \\ \\{ i_1, ..., i_r \\} - \\text{losowy podzbiór obserwacji}$$\n",
    "$$\\theta = \\theta - learning\\_rate * \\frac{d\\tilde{Cost}}{d\\theta_j}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten schemat powtarzamy wielokrotnie tak, żeby każda obserwacja została wykorzystana jeden raz - w praktyce mieszamy losowo kolejność obserwacji i bierzemy kolejne podzbiory - np. dla \"batcha\" wielkości 10, uczymy kolejno na obserwacjach od 1 do 10, od 11 do 20, itd.. Przejście po całych danych to jedna *epoka*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generator(X, y, batch_size=32):\n",
    "    i = 0\n",
    "    n = X.shape[0]\n",
    "    while i < n:        \n",
    "        yield X[i:(i+batch_size)], y[i:(i+batch_size)]        \n",
    "        i += batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# poprzedni przykład, ale z sgd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bg = batch_generator(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 0.2878410941376307\n",
      "Total loss 0.2866995870652562\n",
      "Total loss 0.2866987371096563\n",
      "Total loss 0.2866987362781298\n",
      "Total loss 0.2866987362771672\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n",
      "Total loss 0.28669873627716597\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros(shape=(3,1))\n",
    "b = np.zeros(shape=(1,))\n",
    "\n",
    "for i in range(20):\n",
    "    losses = []\n",
    "    \n",
    "    for X_batch, y_batch in batch_generator(X,y):\n",
    "        #forward pass\n",
    "        out,cache = fc_forward(X_batch,w,b)\n",
    "        \n",
    "        loss,dloss = mse_loss_with_gradient(out,y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #backward\n",
    "        dx, dw, db = fc_backward(dloss,cache)\n",
    "        \n",
    "        w -= dw * learning_rate\n",
    "        b -= db * learning_rate\n",
    "        \n",
    "        \n",
    "    print('Total loss %s' %loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prawdziwe dane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.boston_housing.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = y_train.reshape((-1, 1))\n",
    "y_test = y_test.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 13)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** - dopasuj model analogicznie, jak wcześniej - tym razem stosując SGD. W każdym kroku wylosuj pewną ilość obserwacji ze zbioru treningowego, i dopasuj do nich wagi. Na końcu sprawdź, jaki średni błąd model osiąga na zbiorze testowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 143.77798665060328\n",
      "Total loss 24.72137736371449\n",
      "Total loss 23.508722471525513\n",
      "Total loss 23.146855997603993\n",
      "Total loss 22.99713959802191\n",
      "Total loss 22.931201284115584\n",
      "Total loss 22.900358265500646\n",
      "Total loss 22.885108905235192\n",
      "Total loss 22.877277833484474\n",
      "Total loss 22.87325652366057\n",
      "Test final loss 36.20176374822127\n"
     ]
    }
   ],
   "source": [
    "w = np.zeros(shape=(x_train.shape[1],1))\n",
    "b = np.zeros(shape=(1,))\n",
    "learning_rate = 0.1\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "\n",
    "for i in range(10):\n",
    "    losses = []\n",
    "    \n",
    "    for X_batch, y_batch in batch_generator(x_train_scaled,y_train):\n",
    "        #forward pass\n",
    "        out,cache = fc_forward(X_batch,w,b)\n",
    "        \n",
    "        loss,dloss = mse_loss_with_gradient(out,y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #backward\n",
    "        dx, dw, db = fc_backward(dloss,cache)\n",
    "        \n",
    "        w -= dw * learning_rate\n",
    "        b -= db * learning_rate\n",
    "        \n",
    "    print('Total loss %s' %np.mean(losses))\n",
    " \n",
    "\n",
    "out_test,_ = fc_forward(scaler.transform(x_test),w,b)\n",
    "loss,_ = mse_loss_with_gradient(out_test,y_test)\n",
    "print('Test final loss %s' %loss)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model przeuczony, większy spadek błedu na zbiorze treningowym niż na testowym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wielowarstwowa sieć neuronowa\n",
    "\n",
    "(*Multilayer perceptron*, *feedforward neural network*)\n",
    "\n",
    "\n",
    "<img src=\"Grafika/MLP.jpg\" width=\"700\">\n",
    "Źródło: https://www.intechopen.com/source/html/39071/media/f2.jpg\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 186.8172472414203\n",
      "Total loss 84.15647564261585\n",
      "Total loss 84.12761044517336\n",
      "Total loss 84.1429937709156\n",
      "Total loss 84.14389076749475\n",
      "Total loss 84.14394023541561\n",
      "Total loss 84.14394295541183\n",
      "Total loss 84.14394310494662\n",
      "Total loss 84.14394311316735\n",
      "Total loss 84.14394311361929\n",
      "Test final loss 83.51447023217996\n"
     ]
    }
   ],
   "source": [
    "# spróbujmy z dwoma warstwami\n",
    "hidden_size = 10\n",
    "w1 = np.zeros(shape=(x_train.shape[1],hidden_size))\n",
    "b1 = np.zeros(shape=(hidden_size,))\n",
    "\n",
    "w2 = np.zeros(shape=(hidden_size,1))\n",
    "b2 = np.zeros(shape=(1,))\n",
    "learning_rate = 0.1\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "\n",
    "for i in range(10):# 10 epochs\n",
    "    losses = []\n",
    "    \n",
    "    for X_batch, y_batch in batch_generator(x_train_scaled,y_train):\n",
    "        #forward pass\n",
    "        #step 1\n",
    "        hidden_layer,cache_hidden_layer = fc_forward(X_batch,w1,b1)\n",
    "        #step 2\n",
    "        out,cache = fc_forward(hidden_layer,w2,b2)\n",
    "        \n",
    "        #MSE\n",
    "        loss,dloss = mse_loss_with_gradient(out,y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #backward pass\n",
    "        dhidden, dw2, db2 = fc_backward(dloss,cache)\n",
    "        dx, dw1, db1 = fc_backward(dhidden,cache_hidden_layer)\n",
    "        \n",
    "        w1 -= dw1 * learning_rate\n",
    "        b1 -= db1 * learning_rate\n",
    "        \n",
    "        w2 -= dw2 * learning_rate\n",
    "        b2 -= db2 * learning_rate\n",
    "        \n",
    "    print('Total loss %s' %np.mean(losses))\n",
    " \n",
    "\n",
    "out_hidden_test,_ = fc_forward(scaler.transform(x_test),w1,b1)\n",
    "out_test,_ = fc_forward(out_hidden_test,w2,b2)\n",
    "\n",
    "loss,_ = mse_loss_with_gradient(out_test,y_test)\n",
    "print('Test final loss %s' %loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co tu się wydarzyło?  \n",
    "\n",
    "Jeśli przypomnimy sobie jak wygląda propagacja wsteczna, to dostaniemy prostą odpowiedź. Ponieważ wszystkie wagi zostały zainicjalizowane na 0, to wartość warstwy ukrytej po pierwszym kroku wynosiła 0 na wszystkich elementach. Wobec tego ponieważ pochodna $Y$ po wartościach $W2$ to właśnie nasza warstwa ukryta, to wagi w ogóle się nie uaktualniły. Jedynym parametrem, który miał szansę się zmienić było $b2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([22.55821823])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spróbujmy więc wygenerować wagi losowo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss 478.18106333345565\n",
      "Total loss 194.79127513272243\n",
      "Total loss 25.870363745609968\n",
      "Total loss 23.600303260309484\n",
      "Total loss 23.351114888137623\n",
      "Total loss 23.263061881415016\n",
      "Total loss 23.200175055623355\n",
      "Total loss 23.144733307422843\n",
      "Total loss 23.09617932482865\n",
      "Total loss 23.054319054684388\n",
      "Test final loss 30.990666809281997\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "w1 = np.random.normal(scale = 0.001,size = (x_train.shape[1],hidden_size))\n",
    "b1 = np.random.normal(scale = 0.001,size = (hidden_size,))\n",
    "\n",
    "w2 = np.random.normal(scale = 0.001,size = (hidden_size,1))\n",
    "b2 = np.random.normal(scale = 0.001,size = (1,))\n",
    "learning_rate = 0.01\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "\n",
    "for i in range(10):\n",
    "    # next epoch\n",
    "    losses = []\n",
    "    \n",
    "    for X_batch, y_batch in batch_generator(x_train_scaled,y_train):\n",
    "        #forward pass\n",
    "        #step 1\n",
    "        hidden_layer,cache_hidden_layer = fc_forward(X_batch,w1,b1)\n",
    "        #step 2\n",
    "        out,cache = fc_forward(hidden_layer,w2,b2)\n",
    "        \n",
    "        #MSE\n",
    "        loss,dloss = mse_loss_with_gradient(out,y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #backward pass\n",
    "        dhidden, dw2, db2 = fc_backward(dloss,cache)\n",
    "        dx, dw1, db1 = fc_backward(dhidden,cache_hidden_layer)\n",
    "        \n",
    "        w1 -= dw1 * learning_rate\n",
    "        b1 -= db1 * learning_rate\n",
    "        \n",
    "        w2 -= dw2 * learning_rate\n",
    "        b2 -= db2 * learning_rate\n",
    "        \n",
    "    print('Total loss %s' %np.mean(losses))\n",
    " \n",
    "\n",
    "out_hidden_test,_ = fc_forward(scaler.transform(x_test),w1,b1)\n",
    "out_test,_ = fc_forward(out_hidden_test,w2,b2)\n",
    "\n",
    "loss,_ = mse_loss_with_gradient(out_test,y_test)\n",
    "print('Test final loss %s' %loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zauważmy teraz, że nasz model jest złożeniem dwóch liniowych transformacji ze sobą - czyli tak naprawdę nie wprowadziliśmy żadnej dodatkowej zdolności predykcyjnej dla naszego modelu!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozwiązaniem jest wprowadzenie nieliniowej funkcji aktywacyjnej - dzięki temu nasza dwuwarstwowa sieć staje się uniwersalnym aproksymatorem - tzn dla dowolnej funkcji istnieje taka dwuwarstwowa sieć neuronowa, która jest w stanie ją przybliżyć z dowolnie małym błędem (fakt matematyczny)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcja relu:\n",
    "\n",
    "$$ relu(x) = \\max(0, x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** - zaimplementuj funkcję `relu_forward`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_forward(x):\n",
    "    \"\"\"\n",
    "    Funkcja oblicza wartości macierzy danych x po transformacji przez funkcję aktywacji ReLU.\n",
    "    \n",
    "    Argumenty:\n",
    "    - x: Dane wejściowe - np.array o dwóch wymiarach (N,M)\n",
    "    \n",
    "    Wartość zwracana:\n",
    "    - out: wyjście z warstwy\n",
    "    - cache: krotka zawierająca argumenty wejściowe\n",
    "    \"\"\"\n",
    "    return np.maximum(0,x),x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu_backward(dout, cache):\n",
    "    \"\"\"\n",
    "    Funkcja oblicza 'backward pass' dla aktywacji ReLU.\n",
    "    \n",
    "    Argumenty:\n",
    "    - dout: pochodna wejściowa (czyli pochodna funkcji straty po warstwie leżącej po aktualnej)\n",
    "    - cache: krotka zwrócona przez fc_forward\n",
    "    \n",
    "    Wartość zwracana:\n",
    "    - dx: pochodna po elementach x - np.array o wymiarze (N, M)\n",
    "    \"\"\"\n",
    "    \n",
    "    x = cache\n",
    "    \n",
    "    out = np.maximum(x,0)\n",
    "    out[out>0] = 1 # pochodna relu\n",
    "    \n",
    "    dx = dout * out\n",
    "    \n",
    "    return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test final loss 285.9950356257026\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 10\n",
    "w1 = np.random.normal(scale = 0.001,size = (x_train.shape[1],hidden_size))\n",
    "b1 = np.random.normal(scale = 0.001,size = (hidden_size,))\n",
    "\n",
    "w2 = np.random.normal(scale = 0.001,size = (hidden_size,1))\n",
    "b2 = np.random.normal(scale = 0.001,size = (1,))\n",
    "learning_rate = 0.01\n",
    "\n",
    "scaler = StandardScaler()\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "all_losses_train = []\n",
    "all_losses_test = []\n",
    "\n",
    "for i in range(100):\n",
    "    # next epoch\n",
    "    losses = []\n",
    "    \n",
    "    for X_batch, y_batch in batch_generator(x_train_scaled,y_train):\n",
    "        #forward pass\n",
    "        #step 1\n",
    "        hidden_layer,cache_hidden_layer = fc_forward(X_batch,w1,b1)\n",
    "        #step 2\n",
    "        hidden_after_relu,cache_relu = relu_forward(hidden_layer)\n",
    "        out,cache = fc_forward(hidden_after_relu,w2,b2)\n",
    "        \n",
    "        #MSE\n",
    "        loss,dloss = mse_loss_with_gradient(out,y_batch)\n",
    "        losses.append(loss)\n",
    "        \n",
    "        #backward pass\n",
    "        dhidden, dw2, db2 = fc_backward(dloss,cache)\n",
    "        drelu = relu_backward(dhidden,cache_relu)\n",
    "        dx, dw1, db1 = fc_backward(drelu,cache_hidden_layer)\n",
    "        \n",
    "        w1 -= dw1 * learning_rate\n",
    "        b1 -= db1 * learning_rate\n",
    "        \n",
    "        w2 -= dw2 * learning_rate\n",
    "        b2 -= db2 * learning_rate\n",
    "     \n",
    "    all_losses_train.append(np.mean(losses))\n",
    "    #print('Total loss %s' %np.mean(losses))\n",
    "    \n",
    "    #test\n",
    "    out_hidden_test,_ = fc_forward(x_test_scaled,w1,b1)\n",
    "    hidden_relu_test = relu_forward(out_hidden_test)\n",
    "    out_test,_ = fc_forward(hidden_relu_test,w2,b2)\n",
    "\n",
    "    loss,_ = mse_loss_with_gradient(out_test,y_test)\n",
    "    all_losses_test.append(loss)\n",
    " \n",
    "\n",
    "out_hidden_test,_ = fc_forward(scaler.transform(x_test),w1,b1)\n",
    "hidden_relu_test = relu_forward(out_hidden_test)\n",
    "out_test,_ = fc_forward(hidden_relu_test,w2,b2)\n",
    "\n",
    "loss,_ = mse_loss_with_gradient(out_test,y_test)\n",
    "print('Test final loss %s' %loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** : narysuj wykres, na którym zobaczymy jak zmienia się wartość funkcji straty na zbiorze treningowym i testowym wraz z kolejnymi epokami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(all_losses_train,label = 'train')\n",
    "plt.plot(all_losses_test,label = 'test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jak widać, zbyt długie uczenie prowadzi do modelu, który świetnie radzi sobie na zbiorze treningowym, ale traci zdolność do generalizacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Klasyfikacja - funkcja softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wyobraźmy sobie, że chcemy rozwiązać problem klasyfikacji $n$-klasowej za pomocą sieci neuronowej. W obecnej formie, na wyjściu dostajemy przekształcenie liniowe z ostatniej warstwy ukrytej na warstwę wyjściową (o rozmiarze $n$).  \n",
    "Aby móc dokonać musimy posiadać jakiś mechanizm zamieniający liniowe wyjście z naszej sieci ($(x_1, x_2, \\ldots, x_n)$) na wektor prawdopodobieństw ($(p_1, p_2, \\ldots, p_n)$). Założenie jest takie, że duże wartości wyjścia mają przechodzić na prawdopodobieństwo bliskie 1, a małe (mocno ujemne) - na wartości bliskie 0. Dodatkowo, prawdopodobieństwa wszystkich klas muszą się sumować do 1. Funkcją której szukamy, jest funkcja softmax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcję **softmax** definiujemy następująco - jest to wektor, którego $j$-ta współrzędna wygląda następująco:\n",
    "\n",
    "$$\\sigma(x)_j = p_j =  \\frac{\\exp{x_j}}{\\sum_{i=1}^n \\exp{x_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Widzimy, że funkcja skonstruowana jest w taki sposób, aby spełniać powyższe warunki - najpierw za pomocą funkcji wykładniczej przekształcamy każdy element na przedział $(0, \\infty)$, a następnie dzielimy przez sumę wszystkich elementów. Dzięki temu dostajemy wartości z zakresu $(0, 1)$ sumujące się do 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** - implementacja funkcji `softmax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# implementacja:\n",
    "def softmax(x):\n",
    "    # x - np.array o rozmiarze (N,)\n",
    "    exp_x = np.exp(x)\n",
    "    return  exp_x / exp_x.sum() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mamy już mapowanie które z wartości rzeczywistych $(x_1, x_2, \\ldots, x_n)$ tworzy nam prawdopodobieństwa $(p_1, p_2, \\ldots, p_n)$. Potrzebujemy jeszcze funkcji, która oceni to, jak dobrze prawdopodobieństwa odpowiadają prawdziwym wartościom $(y_1, y_2, \\ldots, y_n)$ (zakładamy tutaj, że labelki mamy w postaci *one-hot*).  \n",
    "Znamy już taką funkcję - jest to *log loss*. Tutaj wprowadzimy jej wieloklasową definicję:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "L(p, y) = - \\sum_{i=1}^{n}(y_i \\cdot \\log(p_i))\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1429316284998995\n",
      "4.142931628499899\n"
     ]
    }
   ],
   "source": [
    "x_example = np.array((1, 3, 5)).reshape(1, 3)\n",
    "y_example = np.array((0,0,1)).reshape(1, 3)\n",
    "y_example_bad = np.array((1,0,0)).reshape(1, 3)\n",
    "print(log_loss(y_pred=softmax(x_example), y_true=y_example))\n",
    "print(log_loss(y_pred=softmax(x_example), y_true=y_example_bad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ostatnim elementem układanki jest pochodna naszej funkcji straty po wartościach wejściowych $(x_1, x_2, \\ldots, x_n)$. Warto tutaj zwrócic uwagę, że policzenie tej pochodnej jest odrobine bardziej skomplikowane, niż się wydaje (ale wciąż proste).  \n",
    "\n",
    "Interesuje nas pochodna \n",
    "$$ \\frac{\\partial L}{\\partial x_i} $$\n",
    "\n",
    "Zauważmy, że $L$ zależy od każdego z $p_j$, a z kolei każde z $p_j$ zależy od naszego z $x_i$ (poprzez czynnik normujący). Możemy więc zapisać naszą funkcję w następujący sposób:\n",
    "\n",
    "$$ L(p_1(x_i), p_2(x_i), \\ldots, p_n(x_i)) $$\n",
    "\n",
    "(oczywiście prawdopodobieństwa $p$ zależą też od pozostałych elementów $x$, ale tutaj upraszczamy zapis, aby móc czytelnie przedstawić naszą zależność.  \n",
    "\n",
    "Pamiętamy z podstaw analizy, że w tej sytuacji pochodna funkcji $L$ po $x_i$ to \n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial p_1} \\frac{\\partial p_1}{\\partial x_i} + \\frac{\\partial L}{\\partial p_2} \\frac{\\partial p_2}{\\partial x_i} + \\ldots + \\frac{\\partial L}{\\partial p_3} \\frac{\\partial p_3}{\\partial x_i}  = \\sum_{j=1}^n \\frac{\\partial L}{\\partial p_j} \\frac{\\partial p_j}{\\partial x_i}$$\n",
    "\n",
    "Widać więc, co musimy policzyć - są to pochodne $\\frac{\\partial L}{\\partial p_j}$ oraz $\\frac{\\partial p_j}{\\partial x_i}$, przy czym ta druga rozbija się na dwa przypadki - gdy $i=j$ i gdy $i \\neq j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Najpierw liczymy pochodną $\\frac{\\partial L}{\\partial p_j}$.  \n",
    "Zauważmy, że funkcja $L$ składa się z $n$ czynników, z czego tylko jeden zależy od $p_j$ - tylko on da nam więc niezerową pochodną.\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial p_j} = - \\frac{\\partial y_j \\cdot \\log(p_j)}{\\partial p_j} = - \\frac{y_j}{p_j}\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Następnie policzymy pochodną $\\frac{\\partial p_j}{\\partial x_i}$ gdy $i=j$.\n",
    "\n",
    "<br>\n",
    "\n",
    "Podstawmy $p_j$ zgodnie ze wzorem, a następnie zastosujmy wzór na różniczkowanie pierwiastka:\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p_i}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\frac{\\exp{x_i}}{\\sum_{k=1}^n \\exp{x_k}} = \n",
    "\\frac{\\exp{x_i} \\cdot \\sum_{k=1}^n \\exp{x_k} - \\exp{x_i} \\cdot \\exp{x_i}}{\\sum_{k=1}^n \\exp{x_k} \\cdot \\sum_{k=1}^n \\exp{x_k}} =\n",
    "\\frac{\\exp{x_i} \\cdot(\\sum_{k=1}^n \\exp{x_k} - \\exp{x_i})}{\\sum_{k=1}^n \\exp{x_k} \\cdot \\sum_{k=1}^n \\exp{x_k}}\n",
    "$$\n",
    "\n",
    "Skorzystaliśmy tutaj z podobnego faktu co wcześniej - wyrażenie $\\sum_{k=1}^n \\exp{x_k}$ składa się z $n$ czynników, z których tylko jeden zależy od $x_i$ - i jego pochodna jest równa 1.\n",
    "\n",
    "\n",
    "Teraz możemy rozbić nasz wynik na kilka mniejszych wyrażeń:\n",
    "\n",
    "$$\n",
    "\\frac{\\exp{x_i} \\cdot(\\sum_{k=1}^n \\exp{x_k} - \\exp{x_i})}{\\sum_{k=1}^n \\exp{x_k} \\cdot \\sum_{k=1}^n \\exp{x_k}} = \\frac{\\exp{x_i} }{\\sum_{k=1}^n \\exp{x_k}} \\frac{\\sum_{k=1}^n \\exp{x_k} - \\exp{x_i}}{\\sum_{k=1}^n \\exp{x_k}} \n",
    "= \\frac{\\exp{x_i} }{\\sum_{k=1}^n \\exp{x_k}} \\left(\\frac{\\sum_{k=1}^n \\exp{x_k}}{\\sum_{k=1}^n \\exp{x_k}}  - \\frac{\\exp{x_i}}{\\sum_{k=1}^n \\exp{x_k}}  \\right)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W tym momencie doznajemy olśnienia - przecież te wyrażenia to wzory na $p_i$! Całe wyrażenie upraszcza się więc do   \n",
    "  \n",
    "$$ p_i (1 - p_i) .$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ostatni krok - pochodna $\\frac{\\partial p_j}{\\partial x_i}$ gdy $i \\neq j$. \n",
    "\n",
    "<br>\n",
    "\n",
    "Tutaj wyrażenie upraszcza się jeszcze bardziej, po pochodna licznika ($x_j$) po $x_i$ jest równa 0:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial p_j}{\\partial x_i} = \\frac{\\partial}{\\partial x_i} \\frac{\\exp{x_j}}{\\sum_{k=1}^n \\exp{x_k}} = \n",
    "\\frac{- \\exp{x_j} \\cdot \\exp{x_i}}{\\sum_{k=1}^n \\exp{x_k} \\cdot \\sum_{k=1}^n \\exp{x_k}} \n",
    "$$\n",
    "\n",
    "Po rozbiciu wyniku na dwa pierwiastki dostajemy:\n",
    "\n",
    "$$\n",
    "\\frac{- \\exp{x_j} \\cdot \\exp{x_i}}{\\sum_{k=1}^n \\exp{x_k} \\cdot \\sum_{k=1}^n \\exp{x_k}}  = \n",
    "-\\frac{\\exp{x_j} }{\\sum_{k=1}^n \\exp{x_k} } \\cdot \\frac{\\exp{x_i}}{\\sum_{k=1}^n \\exp{x_k}} =\n",
    "-p_i \\cdot p_j\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Możemy teraz złożyć całość do kupy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\frac{\\partial L}{\\partial x_i} = \\sum_{j=1}^n \\frac{\\partial L}{\\partial p_j} \\frac{\\partial p_j}{\\partial x_i}  = \\sum_{j=1}^n -\\frac{y_j}{ p_j} \\frac{\\partial p_j}{\\partial x_i} = \n",
    "-\\frac{y_i}{ p_i}\\frac{\\partial p_i}{\\partial x_i} + \\sum_{j \\neq i}-\\frac{y_j}{ p_j} \\frac{\\partial p_j}{\\partial x_i} $$\n",
    "\n",
    "<br>\n",
    "\n",
    "Rozbiliśmy tutaj ostatnią sumę na dwa wyrażenia, abysmy mogli podstawić obie policzone wcześniej pochodne.\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "-\\frac{y_i}{ p_i}\\frac{\\partial p_i}{\\partial x_i} + \\sum_{j \\neq i}-\\frac{y_j}{ p_j} \\frac{\\partial p_j}{\\partial x_i} = -\\frac{y_i}{ p_i} p_i(1-p_i) + \\sum_{j \\neq i}-\\frac{y_j}{ p_j} (-p_i \\cdot p_j) \n",
    "$$\n",
    "\n",
    "Widzimy, że trochę się poskraca:\n",
    "\n",
    "$$\n",
    "-\\frac{y_i}{ p_i} p_i(1-p_i) + \\sum_{j \\neq i}-\\frac{y_j}{ p_j} (-p_i \\cdot p_j)  = \n",
    "-y_i(1-p_i) + \\sum_{j \\neq i} y_j \\cdot p_i =\n",
    "-y_i + p_i \\cdot y_i + p_i \\sum_{j \\neq i}y_j \n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Ostatnie dwa wyrażenie możemy znowu włączyć pod jedną sumę:\n",
    "$$\n",
    "-y_i + p_i \\cdot y_i + p_i \\sum_{j \\neq i}y_j \n",
    "-y_i + p_i \\cdot \\sum_{i=1}^n y_j  = p_i - y_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mamy więc ostateczny wzór na pochodną funkcji straty po i-tej współrzędnej wektora **x**\n",
    "\n",
    "**Przepis jest prosty - od prawdopodobieństwa klasy $i$ odejmujemy wartość indykatora tej klasy.** Ma to intuicyjne wytłumaczenie - jeśli prawdopodobieństwo klasy $k$ jest duże, i ta obserwacja faktycznie pochodzi z klasy $k$, to pochodna powinna być bliska 0. Jeśli natomiast prawdopodobieństwo nie zgadza się z prawdziwą klasą, to pochodna jest istotnie niezerowa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Zadanie ** : zaimplementuj funkcję, która zrealizuje zaaplikowania funkcji softmax oraz policzenia na otrzymanych prawdopodobieństwach funkcję straty *log_loss*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax_loss_with_gradient(x, y):\n",
    "    \"\"\"\n",
    "    Oblicza funkcję straty log_loss z użyciem funkcji softmax dla macierzy predykcji liniowych `x` oraz \n",
    "    prawdziwych wartości y\n",
    "    \n",
    "    Wartości wejściowe:\n",
    "    - x: dane wejściowe - np.array o wymiarach (N, M)\n",
    "    - y: wektor indykatorów prawdziwych klas (czyli one-hot) - np.array o wymiarach (N, M)\n",
    "    \n",
    "    Wartości zwracane:\n",
    "    - loss: wartość funkcji straty\n",
    "    - dx: pochodna funkcji straty po x\n",
    "    \"\"\"\n",
    "    \n",
    "    # najpierw przetransformuj kazdy wiersz x za pomocą funkcji softmax\n",
    "    #probs = np.exp(x )\n",
    "    probs = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    probs /= np.sum(probs, axis=1, keepdims=True)\n",
    "    \n",
    "    # następnie oblicz log_loss na całym zbiorze\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(np.log(probs + 1e-15) * y) / N # log loss uśredniony\n",
    "    \n",
    "    # na koniec oblicz pochodną po x zgodnie z powyższym wzorem\n",
    "    dx = probs.copy()\n",
    "    dx -= y\n",
    "    dx /= N # również musimy uśrednić\n",
    "    \n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# testy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_example = np.array((2, 3, 5)).reshape(1, 3)\n",
    "y_example = np.array((0,0,1)).reshape(1, 3)\n",
    "loss_scikit = log_loss(y_pred=softmax(x_example), y_true=y_example)\n",
    "loss_ours, grad = softmax_loss_with_gradient(x_example, y_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1934897514720433e-15\n"
     ]
    }
   ],
   "source": [
    "print(loss_scikit - loss_ours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.04201007  0.1141952  -0.15620527]]\n"
     ]
    }
   ],
   "source": [
    "print(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Możemy zbudować klasyfikator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_digits, y_digits = load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "X_digits_train, X_digits_test, y_digits_train, y_digits_test = train_test_split(X_digits, y_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# labelki muszą być w formacie kategorycznym\n",
    "y_digits_train_ohe = pd.get_dummies(y_digits_train).values\n",
    "y_digits_test_ohe = pd.get_dummies(y_digits_test).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zadanie** - w sposób analogiczny do poprzednich przykładów zbuduj klasyfikator który będzie rozpoznawał cyfry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_digits_train_scaled = scaler.fit_transform(X_digits_train)\n",
    "X_digits_test_scaled = scaler.transform(X_digits_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss: 2.3034264468914962\n",
      "Total loss: 2.2976665733861856\n",
      "Total loss: 2.1492145116141437\n",
      "Total loss: 1.5149453028291684\n",
      "Total loss: 0.7893097405330862\n",
      "Total loss: 0.45742601680860856\n",
      "Total loss: 0.3110970590212715\n",
      "Total loss: 0.225483378430067\n",
      "Total loss: 0.1765801925562757\n",
      "Total loss: 0.146301099466911\n",
      "Total loss: 0.12555126352530177\n",
      "Total loss: 0.11007261709392074\n",
      "Total loss: 0.0978924083966074\n",
      "Total loss: 0.0879656306420039\n",
      "Total loss: 0.07953942640792025\n",
      "Total loss: 0.07235771373488836\n",
      "Total loss: 0.06601649514155167\n",
      "Total loss: 0.06054097049329948\n",
      "Total loss: 0.05560136346853246\n",
      "Total loss: 0.051284718291876016\n",
      "0.12190162635544326\n",
      "0.9622222222222222\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.1\n",
    "\n",
    "hidden_size = 20\n",
    "batch_size = 64\n",
    "w1 = np.random.normal(size=(X_digits_train_scaled.shape[1], hidden_size), scale=0.001)\n",
    "b1 = np.random.normal(size=(hidden_size,), scale=0.001)\n",
    "\n",
    "w2 = np.random.normal(size=(hidden_size, 10), scale=0.001)\n",
    "b2 = np.random.normal(size=(10,), scale=0.001)\n",
    "\n",
    "for i in range(20):\n",
    "    losses = []\n",
    "    for x_batch, y_batch in batch_generator(X_digits_train_scaled, y_digits_train_ohe):\n",
    "    \n",
    "        out_1, cache_1 = fc_forward(x_batch, w1, b1)\n",
    "        relu_1, relu_cache_1 = relu_forward(out_1)\n",
    "        out_2, cache_2 = fc_forward(relu_1, w2, b2)\n",
    "        loss, dloss = softmax_loss_with_gradient(out_2, y_batch)\n",
    "        losses.append(loss)\n",
    "        # backward pass\n",
    "\n",
    "        dx_2, dw_2, db_2 = fc_backward(dloss, cache_2)\n",
    "        drelu = relu_backward(dx_2, relu_cache_1)\n",
    "        dx_1, dw_1, db_1 = fc_backward(drelu, cache_1)\n",
    "\n",
    "        w1 -= dw_1 * learning_rate\n",
    "        b1 -= db_1 * learning_rate\n",
    "\n",
    "        w2 -= dw_2 * learning_rate\n",
    "        b2 -= db_2 * learning_rate\n",
    "\n",
    "\n",
    "    print('Total loss: %s' % np.mean(losses))\n",
    "        \n",
    "test_hidden_1, _ = fc_forward(X_digits_test_scaled, w1, b1)\n",
    "test_relu_1, _ = relu_forward(test_hidden_1)\n",
    "test_predictions, _ = fc_forward(test_relu_1, w2, b2)\n",
    "test_loss, _ = softmax_loss_with_gradient(test_predictions, y_digits_test_ohe)\n",
    "accuracy = np.mean(test_predictions.argmax(axis=1) == y_digits_test_ohe.argmax(axis=1))\n",
    "\n",
    "print(test_loss)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
