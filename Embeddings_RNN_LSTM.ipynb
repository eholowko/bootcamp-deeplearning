{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddingi\n",
    "\n",
    "Przeanalizujmy co się dzieje w RNN, gdy podajemy słowa w reprezentacji one hot.\n",
    "\n",
    "## $$ h_t = f( W^h * h_{t-1} + W^x * x_t + b)$$\n",
    "\n",
    "Zatem jeśli x to \"one-hot\" z jedynką na pozycji $i$ to:\n",
    "\n",
    "## $$ W^x * x_t = W^x[:,i],  $$\n",
    "\n",
    "Czyli wkład informacji słowa sprowadza się do wzięcia odpowieniej kolumny macierzy wag.\n",
    "\n",
    "Czyli i-ta kolumna macierzy wag jest w pewnym sensie reprezentacją słowa i.\n",
    "\n",
    "Zatem pójdźmy krok dalej: stwórzmy sobie dodatkową warstwę w sieci, zawierającą reprezentacje słów, które będą przekazywane do wyliczenia stanu ukrytego.\n",
    "\n",
    "\n",
    "Wówczas sieć z warstwą \"embeddingów\" ma postać:\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "$x_t$ - id słowa wejściowego w momencie $t$.\n",
    "\n",
    "$EMB$ - macierz embeddingów\n",
    "\n",
    "<br>\n",
    "\n",
    "$$emb_t = EMB[x_t]$$\n",
    "$$ h_t = f( W^h * h_{t-1} + W^x * emb_t + b)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Ta warstwa nazywa się EMBEDDING'ami (embedding layer).\n",
    "\n",
    "\n",
    "<img src=\"Grafika/embeddings.jpg\" width=\"700\">\n",
    "Źródło: https://www.slideshare.net/Geeks_Lab/aibigdata-lab-2016-62764857\n",
    "\n",
    "\n",
    "\n",
    "### Zauważmy, że embeddingi są parametrami sieci, ale jednocześnie reprezentacją słów. Oznacza to, że trenując sieć, uczymy embeddingi, czyli uczymy się reprezentacji słów.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case study: IMBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, SimpleRNN, LSTM, Bidirectional\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5000 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train,y_train),(X_test,y_test) = imdb.load_data(num_words=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 194, 1153, 194, 2, 78, 228, 5, 6, 1463, 4369, 2, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 2, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 2, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 2, 2, 349, 2637, 148, 605, 2, 2, 15, 123, 125, 68, 2, 2, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 2, 5, 2, 656, 245, 2350, 5, 4, 2, 131, 152, 491, 18, 2, 32, 2, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zwróćmy uwagę w powyższym, że ciągi zaczynają się zawsze od \"1\" - jest to oznaczenie początku zdania. Czyli \"początek zdania\" będzie mial swój embedding. Dzięki temu sieć lepiej nauczy się uwzględniać, podczas \"analizy\" pierwszego słow fakt, że to słowo jest pierwsze.\n",
    "\n",
    "Standaryzacja długości sekwencji (znalezienie najdłuższej, wypełnienie zerami pozostałych w taki sposób, aby wszystkie były jednakowej długości)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = sequence.pad_sequences(X_train,maxlen=max_len)\n",
    "X_test = sequence.pad_sequences(X_test,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 400)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = 5000\n",
    "n_test = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[:n_train]\n",
    "y_train = y_train[:n_train]\n",
    "X_test = X_test[:n_test]\n",
    "y_test = y_test[:n_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zwykła sieć rekurencyjna ( z embeddingami)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_dims = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 400, 32)           160000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 100)               13300     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 173,401\n",
      "Trainable params: 173,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(max_features,embeding_dims,input_length=max_len))\n",
    "model.add(SimpleRNN(100,return_sequences=False))# many-to-one\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/100\n",
      " - 26s - loss: 0.6929 - acc: 0.5320 - val_loss: 0.6845 - val_acc: 0.5568\n",
      "Epoch 2/100\n",
      " - 25s - loss: 0.6251 - acc: 0.6997 - val_loss: 0.6542 - val_acc: 0.6056\n",
      "Epoch 3/100\n",
      " - 25s - loss: 0.5279 - acc: 0.7717 - val_loss: 0.6418 - val_acc: 0.6024\n",
      "Epoch 4/100\n",
      " - 26s - loss: 0.5350 - acc: 0.7819 - val_loss: 0.6215 - val_acc: 0.6400\n",
      "Epoch 5/100\n",
      " - 26s - loss: 0.3823 - acc: 0.8344 - val_loss: 0.5384 - val_acc: 0.7504\n",
      "Epoch 6/100\n",
      " - 26s - loss: 0.2556 - acc: 0.9011 - val_loss: 0.5771 - val_acc: 0.7248\n",
      "Epoch 7/100\n",
      " - 25s - loss: 0.1807 - acc: 0.9344 - val_loss: 0.6936 - val_acc: 0.7416\n",
      "Epoch 8/100\n",
      " - 25s - loss: 0.0940 - acc: 0.9680 - val_loss: 0.7869 - val_acc: 0.7256\n",
      "Epoch 9/100\n",
      " - 25s - loss: 0.0469 - acc: 0.9872 - val_loss: 0.7189 - val_acc: 0.7704\n",
      "Epoch 10/100\n",
      " - 26s - loss: 0.0463 - acc: 0.9904 - val_loss: 0.8962 - val_acc: 0.7552\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8f59681c88>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stoping = EarlyStopping(patience=5)\n",
    "model.fit(X_train,y_train,batch_size=32,epochs=100,callbacks=[early_stoping],validation_split=0.25,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.23753432908952235, 0.9356]\n",
      "[0.9043297662734985, 0.7475]\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(X_train,y_train,verbose=2))\n",
    "print(model.evaluate(X_test,y_test,verbose=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple RNN + dense pomiędzy zwracanym stanem ukrytym a outputem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 400, 32)           160000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_5 (SimpleRNN)     (None, 100)               13300     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 178,401\n",
      "Trainable params: 178,401\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(max_features,embeding_dims,input_length=max_len))\n",
    "model2.add(SimpleRNN(100,return_sequences=False))# many-to-one\n",
    "model2.add(Dense(50,activation='tanh'))\n",
    "model2.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model2.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/100\n",
      " - 13s - loss: 0.7024 - acc: 0.5072 - val_loss: 0.6954 - val_acc: 0.5072\n",
      "Epoch 2/100\n",
      " - 12s - loss: 0.7006 - acc: 0.5165 - val_loss: 0.6887 - val_acc: 0.5480\n",
      "Epoch 3/100\n",
      " - 12s - loss: 0.6892 - acc: 0.5349 - val_loss: 0.6845 - val_acc: 0.5544\n",
      "Epoch 4/100\n",
      " - 12s - loss: 0.6668 - acc: 0.5920 - val_loss: 0.6676 - val_acc: 0.5616\n",
      "Epoch 5/100\n",
      " - 12s - loss: 0.6063 - acc: 0.6795 - val_loss: 0.6780 - val_acc: 0.5896\n",
      "Epoch 6/100\n",
      " - 16s - loss: 0.5345 - acc: 0.7189 - val_loss: 0.7133 - val_acc: 0.5824\n",
      "Epoch 7/100\n",
      " - 16s - loss: 0.4854 - acc: 0.7456 - val_loss: 0.7807 - val_acc: 0.5768\n",
      "Epoch 8/100\n",
      " - 14s - loss: 0.4503 - acc: 0.7755 - val_loss: 0.7526 - val_acc: 0.5936\n",
      "Epoch 9/100\n",
      " - 15s - loss: 0.4077 - acc: 0.7915 - val_loss: 0.8054 - val_acc: 0.5976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9c0334b38>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stoping = EarlyStopping(patience=5)\n",
    "model2.fit(X_train,y_train,batch_size=32,epochs=100,callbacks=[early_stoping],validation_split=0.25,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.32980653083324435, 0.846]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.evaluate(X_test,y_test,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dwuwarstwowa sieć rekurencyjna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 400, 32)           160000    \n",
      "_________________________________________________________________\n",
      "simple_rnn_8 (SimpleRNN)     (None, 400, 100)          13300     \n",
      "_________________________________________________________________\n",
      "simple_rnn_9 (SimpleRNN)     (None, 50)                7550      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 180,901\n",
      "Trainable params: 180,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(max_features,embeding_dims,input_length=max_len))\n",
    "model3.add(SimpleRNN(100,return_sequences=True))\n",
    "model3.add(SimpleRNN(50,return_sequences=False))# many-to-one\n",
    "model3.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model3.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "early_stoping = EarlyStopping(patience=5)\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/100\n",
      " - 26s - loss: 0.7007 - acc: 0.5037 - val_loss: 0.6911 - val_acc: 0.5144\n",
      "Epoch 2/100\n",
      " - 31s - loss: 0.6153 - acc: 0.6589 - val_loss: 0.7629 - val_acc: 0.5304\n",
      "Epoch 3/100\n",
      " - 29s - loss: 0.2996 - acc: 0.8824 - val_loss: 0.8891 - val_acc: 0.5312\n",
      "Epoch 4/100\n",
      " - 27s - loss: 0.0761 - acc: 0.9765 - val_loss: 1.5083 - val_acc: 0.5152\n",
      "Epoch 5/100\n",
      " - 31s - loss: 0.0236 - acc: 0.9949 - val_loss: 1.6281 - val_acc: 0.5304\n",
      "Epoch 6/100\n",
      " - 27s - loss: 0.0056 - acc: 0.9992 - val_loss: 1.9399 - val_acc: 0.5232\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9c0e502b0>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(X_train,y_train,batch_size=32,epochs=100,callbacks=[early_stoping],validation_split=0.25,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model uczy się prawie całego tekstu na pamięć"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.9941312866210938, 0.5165]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.evaluate(X_test,y_test,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dwukierunkowa sieć rekurencyjna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 400, 32)           160000    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 400, 200)          26600     \n",
      "_________________________________________________________________\n",
      "simple_rnn_11 (SimpleRNN)    (None, 50)                12550     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 199,201\n",
      "Trainable params: 199,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Embedding(max_features,embeding_dims,input_length=max_len))\n",
    "model4.add(Bidirectional(SimpleRNN(100,return_sequences=True)))\n",
    "model4.add(SimpleRNN(50,return_sequences=False))# many-to-one\n",
    "model4.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model4.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "early_stoping = EarlyStopping(patience=5)\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/100\n",
      " - 54s - loss: 0.7013 - acc: 0.5163 - val_loss: 0.6807 - val_acc: 0.5552\n",
      "Epoch 2/100\n",
      " - 52s - loss: 0.6301 - acc: 0.6480 - val_loss: 0.6615 - val_acc: 0.6312\n",
      "Epoch 3/100\n",
      " - 52s - loss: 0.4729 - acc: 0.7787 - val_loss: 0.7163 - val_acc: 0.6448\n",
      "Epoch 4/100\n",
      " - 53s - loss: 0.3517 - acc: 0.8525 - val_loss: 0.7211 - val_acc: 0.6632\n",
      "Epoch 5/100\n",
      " - 53s - loss: 0.2696 - acc: 0.8957 - val_loss: 0.7957 - val_acc: 0.6848\n",
      "Epoch 6/100\n",
      " - 53s - loss: 0.2045 - acc: 0.9245 - val_loss: 0.8354 - val_acc: 0.6864\n",
      "Epoch 7/100\n",
      " - 52s - loss: 0.1648 - acc: 0.9400 - val_loss: 0.9497 - val_acc: 0.6704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9c0e50550>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.fit(X_train,y_train,batch_size=32,epochs=100,callbacks=[early_stoping],validation_split=0.25,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.004870327949524, 0.651]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model4.evaluate(X_test,y_test,verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na GPU (na przykład w google collab): zamiast `LSTM` -> `CuDNNLSTM`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 400, 32)           160000    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               53200     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 213,301\n",
      "Trainable params: 213,301\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Embedding(max_features,embeding_dims,input_length=max_len))\n",
    "model5.add(LSTM(100,return_sequences=False))# many-to-one\n",
    "model5.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model5.compile(loss='binary_crossentropy',metrics=['accuracy'],optimizer='adam')\n",
    "early_stoping = EarlyStopping(patience=1)\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3750 samples, validate on 1250 samples\n",
      "Epoch 1/100\n",
      " - 57s - loss: 0.6797 - acc: 0.5995 - val_loss: 0.6361 - val_acc: 0.7200\n",
      "Epoch 2/100\n",
      " - 61s - loss: 0.4379 - acc: 0.8123 - val_loss: 0.4132 - val_acc: 0.8336\n",
      "Epoch 3/100\n",
      " - 57s - loss: 0.3032 - acc: 0.8880 - val_loss: 0.4614 - val_acc: 0.7856\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd9b2126208>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(X_train,y_train,batch_size=32,epochs=100,callbacks=[early_stoping],validation_split=0.25,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4555453841686249, 0.791]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.evaluate(X_test,y_test,verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
